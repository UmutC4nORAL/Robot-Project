{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da04f7-4e91-4fa3-91e1-b42873b0741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uoral\\Desktop\\Robot\\ppo_torch.py:138: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
      "C:\\Users\\uoral\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from ppo_torch import Agent\n",
    "from utils import plot_learning_curve\n",
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "    env = gym.make(\"Acrobot-v1\", render_mode=\"human\")\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "    agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, \n",
    "                    alpha=alpha, n_epochs=n_epochs, \n",
    "                    input_dims=env.observation_space.shape)\n",
    "    n_games = 300\n",
    "\n",
    "    figure_file = 'acrobot.png'\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    for i in range(n_games):\n",
    "        observation, info = env.reset()  # Corrected here\n",
    "        done = False\n",
    "        score = 0\n",
    "        history = []\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, truncated, info = env.step(action)  # Moved step_output to this line\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            history.append((observation[0], observation[1]))\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "\n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            \n",
    "            observation = observation_  # Use the updated observation\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n",
    "\n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "                'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "\n",
    "    x = [i + 1 for i in range(len(score_history))]\n",
    "    plot_learning_curve(x, score_history, figure_file)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab37ca-4009-4c4a-ab54-c37b6cc36793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
