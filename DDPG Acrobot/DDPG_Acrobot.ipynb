{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd25096-79a8-4514-be13-a05f1638bd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Excalibur\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -500.0\n",
      "Episode 1, Reward: -500.0\n",
      "Episode 2, Reward: -500.0\n",
      "Episode 3, Reward: -500.0\n",
      "Episode 4, Reward: -500.0\n",
      "Episode 5, Reward: -500.0\n",
      "Episode 6, Reward: -500.0\n",
      "Episode 7, Reward: -500.0\n",
      "Episode 8, Reward: -500.0\n",
      "Episode 9, Reward: -500.0\n",
      "Episode 10, Reward: -500.0\n",
      "Episode 11, Reward: -500.0\n",
      "Episode 12, Reward: -500.0\n",
      "Episode 13, Reward: -500.0\n",
      "Episode 14, Reward: -500.0\n",
      "Episode 15, Reward: -500.0\n",
      "Episode 16, Reward: -500.0\n",
      "Episode 17, Reward: -500.0\n",
      "Episode 18, Reward: -500.0\n",
      "Episode 19, Reward: -500.0\n",
      "Episode 20, Reward: -500.0\n",
      "Episode 21, Reward: -500.0\n",
      "Episode 22, Reward: -500.0\n",
      "Episode 23, Reward: -500.0\n",
      "Episode 24, Reward: -500.0\n",
      "Episode 25, Reward: -500.0\n",
      "Episode 26, Reward: -500.0\n",
      "Episode 27, Reward: -500.0\n",
      "Episode 28, Reward: -500.0\n",
      "Episode 29, Reward: -500.0\n",
      "Episode 30, Reward: -500.0\n",
      "Episode 31, Reward: -500.0\n",
      "Episode 32, Reward: -500.0\n",
      "Episode 33, Reward: -500.0\n",
      "Episode 34, Reward: -500.0\n",
      "Episode 35, Reward: -500.0\n",
      "Episode 36, Reward: -500.0\n",
      "Episode 37, Reward: -500.0\n",
      "Episode 38, Reward: -500.0\n",
      "Episode 39, Reward: -500.0\n",
      "Episode 40, Reward: -500.0\n",
      "Episode 41, Reward: -500.0\n",
      "Episode 42, Reward: -500.0\n",
      "Episode 43, Reward: -500.0\n",
      "Episode 44, Reward: -500.0\n",
      "Episode 45, Reward: -500.0\n",
      "Episode 46, Reward: -500.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 165\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# DDPG güncellemesi\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer\u001b[38;5;241m.\u001b[39mstorage) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m--> 165\u001b[0m     \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 101\u001b[0m, in \u001b[0;36mupdate\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# Replay buffer'dan örnek al\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     state, action, reward, next_state, done \u001b[38;5;241m=\u001b[39m \u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# Kritik ağın kaybını hesapla\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     target_q \u001b[38;5;241m=\u001b[39m critic_target(next_state, actor_target(next_state))\n",
      "Cell \u001b[1;32mIn[1], line 74\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     67\u001b[0m     next_states\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(next_state, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m     68\u001b[0m     dones\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(done, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m     71\u001b[0m     torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(states)),\n\u001b[0;32m     72\u001b[0m     torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(actions)),\n\u001b[0;32m     73\u001b[0m     torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(rewards))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m---> 74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(dones))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     76\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# Ortamı başlatın\n",
    "env = gym.make(\"Acrobot-v1\", render_mode=\"human\")\n",
    "\n",
    "# Sürekli aksiyonları ayrık aksiyonlara eşlemek için bir fonksiyon tanımlayın\n",
    "discrete_to_continuous = {\n",
    "    0: np.array([-1.0]),  # Sol\n",
    "    1: np.array([0.0]),   # Hareket yok\n",
    "    2: np.array([1.0])    # Sağ\n",
    "}\n",
    "\n",
    "# Actor ve Critic sinir ağları\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = torch.relu(self.layer1(state))\n",
    "        a = torch.relu(self.layer2(a))\n",
    "        return self.max_action * torch.tanh(self.layer3(a))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q = torch.relu(self.layer1(torch.cat([state, action], 1)))\n",
    "        q = torch.relu(self.layer2(q))\n",
    "        return self.layer3(q)\n",
    "\n",
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, transition):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "        for i in ind:\n",
    "            state, action, reward, next_state, done = self.storage[i]\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(np.array(reward, copy=False))\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(np.array(done, copy=False))\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.FloatTensor(np.array(actions)),\n",
    "            torch.FloatTensor(np.array(rewards)).unsqueeze(1),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(np.array(dones)).unsqueeze(1)\n",
    "        )\n",
    "\n",
    "# Parametreler ve modeller\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = 1  # Tek sürekli aksiyon\n",
    "max_action = 1.0\n",
    "\n",
    "actor = Actor(state_dim, action_dim, max_action)\n",
    "critic = Critic(state_dim, action_dim)\n",
    "actor_target = Actor(state_dim, action_dim, max_action)\n",
    "critic_target = Critic(state_dim, action_dim)\n",
    "\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-5)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-4)\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "discount = 0.99\n",
    "tau = 0.005\n",
    "\n",
    "# DDPG güncelleme fonksiyonu\n",
    "def update(batch_size=256):\n",
    "    # Replay buffer'dan örnek al\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Kritik ağın kaybını hesapla\n",
    "    target_q = critic_target(next_state, actor_target(next_state))\n",
    "    target_q = reward + ((1 - done) * discount * target_q).detach()\n",
    "\n",
    "    current_q = critic(state, action)\n",
    "    critic_loss = nn.MSELoss()(current_q, target_q)\n",
    "\n",
    "    # Kritik ağı güncelle\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    # Aktör kaybı ve güncellemesi\n",
    "    actor_loss = -critic(state, actor(state)).mean()\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    # Hedef ağları güncelle\n",
    "    for param, target_param in zip(actor.parameters(), actor_target.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    for param, target_param in zip(critic.parameters(), critic_target.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "# Eğitim döngüsü\n",
    "max_episodes = 5000\n",
    "max_steps = 1000\n",
    "batch_size = 256\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Eğer state bir tuple ise, yalnızca ilk öğeyi al\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]\n",
    "    state = np.array(state)  # NumPy dizisine dönüştür\n",
    "\n",
    "    episode_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        # Aktör ağından sürekli aksiyon alın\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action_cont = actor(state_tensor).detach().numpy()[0]\n",
    "\n",
    "        # Sürekli aksiyonu ayrık aksiyona dönüştür\n",
    "        action_disc = int(np.argmax(np.abs(action_cont)))\n",
    "\n",
    "        # Ortamla etkileşim\n",
    "        next_state, reward, terminated, truncated, info = env.step(action_disc)\n",
    "        done = terminated or truncated\n",
    "        if isinstance(next_state, tuple):\n",
    "            next_state = next_state[0]\n",
    "        next_state = np.array(next_state)  # NumPy dizisine dönüştür\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Belleğe ekleyin\n",
    "        replay_buffer.add((state, action_cont, reward, next_state, float(done)))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # DDPG güncellemesi\n",
    "        if len(replay_buffer.storage) > batch_size:\n",
    "            update(batch_size)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "# Eğitim tamamlandıktan sonra, modelin kaydedilmesi\n",
    "torch.save(actor.state_dict(), \"actor_model_final_acrobot.pth\")\n",
    "torch.save(critic.state_dict(), \"critic_model_final_acrobot.pth\")\n",
    "print(\"Final model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
