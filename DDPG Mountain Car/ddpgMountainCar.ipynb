{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1070d62d-9af1-421c-b4b7-fece9e054469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uoral\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -200.0\n",
      "Episode 1, Reward: -200.0\n",
      "Episode 2, Reward: -200.0\n",
      "Episode 3, Reward: -200.0\n",
      "Episode 4, Reward: -200.0\n",
      "Episode 5, Reward: -200.0\n",
      "Episode 6, Reward: -200.0\n",
      "Episode 7, Reward: -200.0\n",
      "Episode 8, Reward: -200.0\n",
      "Episode 9, Reward: -200.0\n",
      "Episode 10, Reward: -200.0\n",
      "Episode 11, Reward: -200.0\n",
      "Episode 12, Reward: -200.0\n",
      "Episode 13, Reward: -200.0\n",
      "Episode 14, Reward: -200.0\n",
      "Episode 15, Reward: -200.0\n",
      "Episode 16, Reward: -200.0\n",
      "Episode 17, Reward: -200.0\n",
      "Episode 18, Reward: -200.0\n",
      "Episode 19, Reward: -200.0\n",
      "Episode 20, Reward: -200.0\n",
      "Episode 21, Reward: -200.0\n",
      "Episode 22, Reward: -200.0\n",
      "Episode 23, Reward: -200.0\n",
      "Episode 24, Reward: -200.0\n",
      "Episode 25, Reward: -200.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 165\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# DDPG güncellemesi\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer\u001b[38;5;241m.\u001b[39mstorage) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m--> 165\u001b[0m     update(batch_size)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 113\u001b[0m, in \u001b[0;36mupdate\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m    111\u001b[0m critic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    112\u001b[0m critic_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 113\u001b[0m critic_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Aktör kaybı ve güncellemesi\u001b[39;00m\n\u001b[0;32m    116\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mcritic(state, actor(state))\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:469\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    468\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;66;03m# call optimizer step pre hooks\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pre_hook \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[0;32m    472\u001b[0m         _global_optimizer_pre_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_pre_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    474\u001b[0m     ):\n\u001b[0;32m    475\u001b[0m         result \u001b[38;5;241m=\u001b[39m pre_hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\profiler.py:688\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_enter_new(\n\u001b[0;32m    689\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\n\u001b[0;32m    690\u001b[0m     )\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[1;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# Ortamı başlatın\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "# Sürekli aksiyonları ayrık aksiyonlara eşlemek için bir fonksiyon tanımlayın\n",
    "discrete_to_continuous = {\n",
    "    0: np.array([-1.0]),  # Sol\n",
    "    1: np.array([0.0]),   # Hareket yok\n",
    "    2: np.array([1.0])    # Sağ\n",
    "}\n",
    "\n",
    "# Actor ve Critic sinir ağları\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = torch.relu(self.layer1(state))\n",
    "        a = torch.relu(self.layer2(a))\n",
    "        return self.max_action * torch.tanh(self.layer3(a))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q = torch.relu(self.layer1(torch.cat([state, action], 1)))\n",
    "        q = torch.relu(self.layer2(q))\n",
    "        return self.layer3(q)\n",
    "\n",
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, transition):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "        for i in ind:\n",
    "            state, action, reward, next_state, done = self.storage[i]\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(np.array(reward, copy=False))\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(np.array(done, copy=False))\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.FloatTensor(np.array(actions)),\n",
    "            torch.FloatTensor(np.array(rewards)).unsqueeze(1),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(np.array(dones)).unsqueeze(1)\n",
    "        )\n",
    "\n",
    "# Parametreler ve modeller\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = 1  # Tek sürekli aksiyon\n",
    "max_action = 1.0\n",
    "\n",
    "actor = Actor(state_dim, action_dim, max_action)\n",
    "critic = Critic(state_dim, action_dim)\n",
    "actor_target = Actor(state_dim, action_dim, max_action)\n",
    "critic_target = Critic(state_dim, action_dim)\n",
    "\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-5)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-4)\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "discount = 0.99\n",
    "tau = 0.005\n",
    "\n",
    "# DDPG güncelleme fonksiyonu\n",
    "def update(batch_size=256):\n",
    "    # Replay buffer'dan örnek al\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Kritik ağın kaybını hesapla\n",
    "    target_q = critic_target(next_state, actor_target(next_state))\n",
    "    target_q = reward + ((1 - done) * discount * target_q).detach()\n",
    "\n",
    "    current_q = critic(state, action)\n",
    "    critic_loss = nn.MSELoss()(current_q, target_q)\n",
    "\n",
    "    # Kritik ağı güncelle\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    # Aktör kaybı ve güncellemesi\n",
    "    actor_loss = -critic(state, actor(state)).mean()\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    # Hedef ağları güncelle\n",
    "    for param, target_param in zip(actor.parameters(), actor_target.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    for param, target_param in zip(critic.parameters(), critic_target.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "# Eğitim döngüsü\n",
    "max_episodes = 5000\n",
    "max_steps = 1000\n",
    "batch_size = 256\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Eğer state bir tuple ise, yalnızca ilk öğeyi al\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]\n",
    "    state = np.array(state)  # NumPy dizisine dönüştür\n",
    "\n",
    "    episode_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        # Aktör ağından sürekli aksiyon alın\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action_cont = actor(state_tensor).detach().numpy()[0]\n",
    "\n",
    "        # Sürekli aksiyonu ayrık aksiyona dönüştür\n",
    "        action_disc = int(np.argmax(np.abs(action_cont)))\n",
    "\n",
    "        # Ortamla etkileşim\n",
    "        next_state, reward, terminated, truncated, info = env.step(action_disc)\n",
    "        done = terminated or truncated\n",
    "        if isinstance(next_state, tuple):\n",
    "            next_state = next_state[0]\n",
    "        next_state = np.array(next_state)  # NumPy dizisine dönüştür\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Belleğe ekleyin\n",
    "        replay_buffer.add((state, action_cont, reward, next_state, float(done)))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # DDPG güncellemesi\n",
    "        if len(replay_buffer.storage) > batch_size:\n",
    "            update(batch_size)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "# Eğitim tamamlandıktan sonra, modelin kaydedilmesi\n",
    "torch.save(actor.state_dict(), \"actor_model_final.pth\")\n",
    "torch.save(critic.state_dict(), \"critic_model_final.pth\")\n",
    "print(\"Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612733c-553f-45cc-925e-c01c458c5776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
